\documentclass[11pt,a4paper,oneside]{report}   
\usepackage{listings}
\linespread{1.5}
\input{preamble}
\usepackage{graphicx}
\graphicspath{ {./images/} }

\title{\Huge{MSc - Media and Textmining}\\Homework results}
\author{\huge{Daniel Mark Kiss}}
\date{2023}

\begin{document}

\maketitle
\newpage
\tableofcontents
\pagebreak

\chapter{Task description}

The data set contains small black and white images labelled with character identifier. The task is to recognize the character (0-9, a-z, A-Z) of unknown images in the test set by deep learning classifier without any human activity (human’s help is equivalent to cheating).
Work out a deep learning classifier model, and train this model by the training dataset! Use cross-validation in order to measure the goodness indicators (accuracy, AUC) of your model! Please pay attention to the parameter optimization! Please investigate the model, the result and describe the details of your solution!

\chapter{Ellaborated work}
In the chapter I will present the ellaborated work that I have done.
For my work I have created separate classes and files for the different subtasks of the Homework.
In every section I will present and explain what the different classses are used for.
\section{program.py}

Program.py is the core of the program. It is the main file that is used to run the program. It is responsible for the following tasks:
\begin{enumerate}
  \item Initializing the FileReader class
  \item Running the FileReader class functions
  \item[--] read\_training\_files()
  \item[--] create\_train\_set()
  \item Initializing the ConvolutionalNeuralNetwork class
  \item Running the ConvolutionalNeuralNetwork class functions
  \item[--]split()
  \item[--] compile()
  \item[--] train()
  \item[--] evaluate()
  \item Reading the test files
  \item Creating the test set
  \item Exporting results to output.txt file
\end{enumerate}
When we would like to run the program we should also import the FileReader and ConvolutionalNeuralNetwork classes.
For running we should type python program.py to the terminal and the program should start running.

\section{cnn.py}

This file contains the fundamental code for the neural network. I used many other libraries for my homework like \emph{numpy}, \emph{tensorflow}, and \emph{sklearn} also.

\subsection{init}
In the \_\_init\_\_ function I initialize the arrays which I used for traing and spliting. These are the different variables that I used in my class:
\begin{itemize}
  \item self.images: contains the raw images
  \item self.label: contains the labels for the images
  \item self.img\_train: contains the train set of images
  \item self.img\_test: contains the test set of images.
  \item self.label\_train: contains the labels of the train set
  \item self.label\_test: contains the labels of the test set
  \item self.model: is a model used from tensorflow.keras.It has five layers, three 2D convolutional layers  and two 2D MaxPooling layers.
        The first convolutional layer has 32 filters/kernels. Each filter is 3x3 matrix. I choose ReLU as the activation function.
        The input shape is 128x128, 1 which is set because the given images are this size and they are grey scaled.
        The MaxPooling layers are used for down-sampling and reducing the spatial dimensions of the input.
        It uses a 2x2 pooling window/filter. It also helps reducing the chnace of overfitting.It only retains only the most important informaton from it.
\end{itemize}
After the Initialization of the model I add a Flatten layer to it.It is used for converting and outputing a 1D array.
It is necessary before passing to a dense, fully connected layer. The next line adds a layer with 64 units (neurons) and a ReLU activation function.
A dense layer means that each neuron in this layer is connected to every neuron in the previous layer. The last line in the init function adds another dense layer with 62 units, representing the output layer.
The activation function used in the output layer is "softmax." Softmax is often used for multi-class classification problems. It converts the raw output scores into probabilities, where each value represents the probability of the input belonging to a particular class.
The sum of all probabilities for a given input is 1, making it suitable for classification tasks like this task also.
\subsection{split}
The split function is used for spliting the images and the labels into traing and testing set, with the test set is 33\% of the input.
I also add a random\_state seed so it will result alawys the same output.

\subsection{compile}

The compile function is used for compiling the model. I used the Adam optimizer for the compilation.
It is an adaptive learning rate optimization algorithm that’s been designed specifically for training deep neural networks.

\subsection{train}

The train function is used for training the model. I used 15 epochs for the training.
I also added a LearningRateScheduler to the training. It is used for changing the learning rate during the training.
I used 64 as the batch size. The batch size is a number of samples processed before the model is updated.
The validation split is set to 0.2. It means that 20\% of the training data is used for validation.

\subsection{evaluate}

The evaluate function is used for evaluating the model. It returns the loss and the accuracy of the model.

\subsection{predict}

The predict function is used for predicting the labels of the test set. It returns the predicted labels.
It only processes one image and gives a prediction for it. It is used for the output.txt file.

\section{labeler.py}

The labeler.py file is used for labeling the images. It is only a huge dictionary with the labels for the images.

\section{filereader.py}

The filereader.py file is used for reading the images and the labels from the given files.
\subsection{read\_training\_files}
It reads the folders and images recursively.
These functions also used for reading not only the training folders but the test folder as well.

\subsection{create\_train\_set}
For every image that has been read it is processed by a private function called \_process\_image().
These processed images are added to the images array and the labels are added to the labels array.
The \_process\_image() function is used for creating the labels for the images extracted from the filename.
Also the images are converted to numpy arrays.
After that the function is normalizing the images.

\section{Tensorflow using GPU}

I used tensorflow-gpu for my homework.
It is a version of tensorflow that uses the GPU for the calculations. I had an interesting problem with it.When I ran the same code on CPU and GPU I got different results.
On CPU the model gave a 87\% accuracy, but on GPU it gave a 75\% accuracy.
\begin{figure}[h]
  \caption{CPU vs GPU}
  \centering
  \includegraphics[width=\textwidth]{cpuvsgpu}
\end{figure}
I tried to solve this problem but I could not find the solution for it.

\chapter{Results and parameter optimization}

In this chapter I will showcase the results of my homework and the parameter optimization that I have done.
As I already mentioned I tried playing with the GPU execution of the CNN which resulted in a lower accuracy but it gave me a faster execution time. On CPU one epoch calculation took nearly 3 minutes in contrast on GPU it only took 40 sec.
On the other hand I tried to play with the batch size and the epochs.

For the first run on the CPU it gave me a 87\% accuracy with 5 epochs and 64 batch size. After this I started the transition to GPU.
On GPU I tried to run the same code with the same parameters but it gave me a 75\% accuracy. I tried to play with the batch size and the epochs.
The third try I have changed and increased the epoch size from 5 to 10. It resulted an increase in the accuracy to 80\%.
The fourth try I have changed the epoch size from 10 to 15. It resulted an increase in the accuracy to 81\%.
The next I tried adding a LearningRateScheduler to the training. It resulted an increase in the accuracy to 83,6\%. I used a logic where if the epoch is smaller than 5 then it uses the normal learning rate and after that using a lr * tf.math.exp(-0.1).
I also tried using differnt learning rate. Increasing it to 0,002 resulted in a 83,7\% accuracy. Decreasing it to 0,0005 resulted in a 82,1\% accuracy.
An other bigger leap was when I adjusted the learning rate with treshold to 3 for the epoch and using a lr * tf.math.exp(-0.5) calculation. It resulted in a 84,2\% accuracy.
Decreasing the batch size to 32 resulted in a 84,7\% accuracy. I also tried increasing the batch size to 128 but it resulted in only a 82,5\% accuracy.

\begin{figure}[h]
  \caption{84,7\% accuracy with 32 batch size}
  \centering
  \includegraphics[width=\textwidth]{best}
\end{figure}

\chapter{Conclusion}

In my homework I have created a convolutional neural network for recognizing the characters of the given images.
I have used tensorflow and keras for the implementation. I have also used numpy and sklearn for the implementation. The  best accuracy that I have achieved was 84,7\%.
Unfortunately I could not achieve a higher accuracy. I have tried to play with the parameters but I could not find the best combination for the parameters. Maybe with more experience I would be able to achieve a higher accuracy.

F1 macro-average: 0.753
Accuracy: 0.788

\end{document}
